[RL]
block            = "MC"
# 算法名称
algo             = 'DQN'
# 环境名称
env              = 'Sitian'
# 保存结果的路径
result_path      = curr_path + "/outputs/" + self.env + '/' + curr_time + '/' + self.block + '/results/'
# 保存模型的路径
model_path       = curr_path + "/outputs/" + self.env + '/' + curr_time + '/' + self.block + '/models/'
# 训练的回合数
train_eps        = 80
# 测试的回合数
eval_eps         = 20

[DQN]
# 强化学习中的折扣因子
gamma            = 0.95
# e-greedy策略中初始epsilon
epsilon_start    = 0.99
# e-greedy策略中的终止epsilon
epsilon_end      = 0.001
# e-greedy策略中epsilon的衰减率
epsilon_decay    = 3000
# 学习率
lr               = 0.001
# 经验回放的容量
memory_capacity  = 1000000
# mini-batch SGD中的批量大小
batch_size       = 16
# 目标网络的更新频率
target_update    = 8
# hidden size of net
hidden_dim       = 256